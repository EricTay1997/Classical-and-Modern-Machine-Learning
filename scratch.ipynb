{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b7ae1c-5843-44ab-8b21-d7de06e1af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "805cd6a1-ae29-47ba-a722-1a6f8e38ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob('/Users/erict/Desktop/Classical-and-Modern-Machine-Learning/classical/*') + glob.glob('/Users/erict/Desktop/Classical-and-Modern-Machine-Learning/modern/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a641e67f-875a-4db0-b5d8-061648c75469",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folders:\n",
    "    output_file = f\"{folder}/notes.md\"\n",
    "    ! cp /Users/erict/Desktop/Classical-and-Modern-Machine-Learning/notes.md {output_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e7f77f0-7c24-4f51-b1c3-d1b1bfb4b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folders:\n",
    "    output_file = f\"{folder}/code.ipynb\"\n",
    "    ! cp /Users/erict/Desktop/Classical-and-Modern-Machine-Learning/code.ipynb {output_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d4ea9f-9415-4e18-b0d8-484806d31e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    \"basics\", \"coding_practices\", \"cnns\", \"rnns_lstms\", \"attention_transformers\", \"autoencoders\", \"gans\", \n",
    "    \"diffusion\", \"gnns\", \"flows\", \"meta_learning\", \"contrastive_learning\", \"optimization\", \"computer_vision\", \"nlp\", \n",
    "    \"recommender_systems\", \"rl\", \"hyperparameter_optimization\", \"compuational_performance\", \"safety\", \"interpretability\", \n",
    "    \"applied\", \"misc\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa29a62d-9e9f-4e47-99e5-a252fe2e4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/erict/Desktop/Classical-and-Modern-Machine-Learning/modern/01_basics/notes.md\"\n",
    "for i in range(2, 24):\n",
    "    folder = \"/Users/erict/Desktop/Classical-and-Modern-Machine-Learning/modern/\" + format(i, \"02d\") + \"_\" + folders[i-1]\n",
    "    output_file = f\"{folder}/notes.md\"\n",
    "    ! cp {input_file} {output_file}\n",
    "    # ! rm -rf {output_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693901ab-70cd-455f-a9ed-29790401df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "## Commonly Used Functions\n",
    "- Softplus: $\\zeta(x) = \\log(1+\\exp(x))$, a smoothed version of $x^+ = \\max(0, x)$\n",
    "- Sigmoid: $\\frac{d}{d x} \\zeta(x)=\\sigma(x) = \\frac{1}{1+\\exp (-x)}$\n",
    "- Additional properties of softplus and sigmoid:\n",
    "  - $ \\zeta(x)=\\int_{-\\infty}^x \\sigma(y) d y$\n",
    "  - $ \\frac{d}{d x} \\sigma(x)=\\sigma(x)(1-\\sigma(x))$\n",
    "  - $ 1-\\sigma(x)=\\sigma(-x)$\n",
    "  - $ \\log \\sigma(x)=-\\zeta(-x)$\n",
    "  - $ \\forall x \\in(0,1), \\sigma^{-1}(x)=\\log \\left(\\frac{x}{1-x}\\right)$\n",
    "  - $ \\zeta^{-1}(x)=\\log (\\exp (x)-1)$\n",
    "  - $ \\zeta(x)-\\zeta(-x)=x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23974c86-f270-4889-b92a-61b4598ec5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concepts (Misc)\n",
    "\n",
    "## The Curse of Dimensionality\n",
    "- As the number of dimensions increase, the \"space\" between training points increases significantly\n",
    "\n",
    "## Manifold Learning\n",
    "- The manifold interpretation is that natural data forms lower-dimensional manifolds in its embedding space. \n",
    "  - I view this as the motivation for why we \"encode\" data into a lower-dimensional space. \n",
    "- [Neural Networks, Manifolds, and Topology](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) mentions the inverse: All $n$-dimensional manifolds can be untangled in $2n+2$ dimensions. \n",
    "  - I wonder if this is good intuition for first increasing the number of hidden units and then going back down? Like the MLP layers of a transformer...\n",
    "\n",
    "## Local Constancy and Smoothness Regularization\n",
    "- Many ML models operate under the prior that the function we want to learn should not change very much within a small region. \n",
    "- Consider now a checkerboard - are these assumptions sufficient?\n",
    "- Is it possible to represent a complicated function efficiently, and is it possible for an estimated function to generalize well to new inputs? Yes!\n",
    "  - A large number of regions $O(2^k)$ can be defined with $O(k)$ examples, so long as we introduce dependencies between the regions through additional assumptions about hte underlying data-generating process.\n",
    "  - Task-specific assumptions could also be appropriate, such as assuming that the target function is periodic.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
