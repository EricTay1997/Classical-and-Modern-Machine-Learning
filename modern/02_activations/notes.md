# Activation Functions
- ![activations.png](activations.png)[Source](https://medium.com/the-modern-scientist/an-overview-of-activation-functions-in-deep-learning-97a85ac00460)
- Swish
  - $\frac{x}{1+e^{-\beta x}}$
- GeLU
  - $xP(X \leq x)$ for $X \sim \mathcal{N}(0,1)$
  - ![gelu.png](gelu.png)[Source](https://paperswithcode.com/method/gelu)