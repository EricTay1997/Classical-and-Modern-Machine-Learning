# Bayesian Statistics

## Frequentist vs Bayesian interpretation
- A frequentist perspective is that the true parameter value $\pmb{\theta}$ is fixed but unknown, while the point estimate $\hat{\pmb{\theta}}$ is a RV since it is a function of the dataset (which is seen as random).
- The Bayesian perspective is that the data is directly observed and so is not random. On the other hand, the true parameter $\pmb{\theta}$ is unknown/uncertain and thus is represented as a random variable. 
  - Ultimately, the Bayesian framework uses the **likelihood** of the data to update the **prior** distribution, obtaining the **posterior** distribution. 
  - Now, while a frequentist may address the uncertainty of $\hat{\pmb{\theta}}$ by calculating its variance, a Bayesian statistician can now reference this posterior distribution, which can of course be used to calculate variance.

## Common Probability Distributions

### Discrete

| Distribution      | Support                                     | $\pmb{\theta}$ | $P(X = x)$                                                     | Mean               | Variance             | Interpretation                                                                                                                                   | $\pmb{\hat{\theta}}_{MLE}$ | $\mathcal{I}(\pmb{\theta})$ | $M_X(t)$                                |
|-------------------|---------------------------------------------|----------------|----------------------------------------------------------------|--------------------|----------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------|-----------------------------|-----------------------------------------|
| Bernoulli         | {0,1}                                       | $p$            | $p^x(1-p)^{1-x}$                                               | $p$                | $p(1-p)$             | Outcome of 1 coin flip.                                                                                                                          | $\bar{X}$                  | $\frac{1}{p(1-p)}$          | $(1-p)+pe^t$                            |
| Binomial          | {$0,1,\ldots,n$}                            | $p$            | $\binom{n}{x} p^x(1-p)^{n-x}$                                  | $np$               | $np(1-p)$            | Number of heads after $n$ coin flips.                                                                                                            | $\frac{\bar{X}}{n}$        | $\frac{1}{p(1-p)}$          | $((1-p)+pe^t)^n$                        |
| Negative Binomial | {$0,1,\ldots,\infty$}                       | $p$            | $\binom{x+r-1}{x} p^r(1-p)^{x}$                                | $\frac{r(1-p)}{p}$ | $\frac{r(1-p)}{p^2}$ | Number of failures before $r$ successes.                                                                                                         |                            | $\frac{r}{p^2(1-p)}$        | $\left( \frac{p}{1-(1-p)e^t} \right)^r$ |
| Multinomial       | $x_i \in\{0,1,2, \ldots, n\}, \sum_i x_i=n$ | $\mathbf{p}$   | $\frac{n!}{x_{1}!x_{2}!\ldots x_{n}!} \prod_{i=1}^n p_i^{x_i}$ |                    |                      |                                                                                                                                                  |                            |                             |                                         |
| Poisson           | {$0,1,\ldots,\infty$}                       | $\lambda$      | $e^{-\lambda}\frac{\lambda^x}{x!} $                            | $\lambda$          | $\lambda$            | The number of events to occur in a fixed amount of time. If this number $\sim Pois(\lambda t)$, then the inter-arrival times $\sim Exp(\lambda)$ | $\bar{X}$                  | $\frac{1}{\lambda}$         | $\exp(\lambda(e^t-1))$                  |
| Geometric         | {$1,2,\ldots,\infty$}                       | $p$            | $(1-p)^{x-1}p$                                                 | $\frac{1}{p}$      | $\frac{1-p}{p^2}$    | The number of trials to get one success.                                                                                                         | $\frac{n}{\sum X_i}$       | $\frac{1}{p^2(1-p)}$        | $\frac{p e^{t}}{1-(1-p) e^{t}}$         |

### Continuous

| Distribution        | Range                                  | $\pmb{\theta}$      | $p(x) $                                                                                                                                                                | Mean                            | Variance                                               | Interpretation                                                                                                                                                                                                  | $\pmb{\hat{\theta}}_{MLE}$                                                                                                                    | $\mathcal{I}(\pmb{\theta})$                                                                  | $M_X(t)$                                                                                        |
|---------------------|----------------------------------------|---------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------|--------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|
| Uniform             | $[a,b]$                                | $a,b$               | $\frac{1}{b-a}$                                                                                                                                                        | $\frac{a+b}{2}$                 | $\frac{(b-a)^2}{12}$                                   |                                                                                                                                                                                                                 | $\min(X_i)$, $\max(X_i)$                                                                                                                      |                                                                                              | $\frac{e^{ta}-e^{ta}}{b-a}$                                                                     |
| Exponential         | $[0,\infty)$                           | $\lambda$           | $\lambda e^{-\lambda x}$                                                                                                                                               | $\frac{1}{\lambda}$             | $\frac{1}{\lambda^2}$                                  | See Poisson                                                                                                                                                                                                     | $\frac{n}{\sum X_i}$                                                                                                                          | $\frac{1}{\lambda^2}$                                                                        | $\frac{\lambda}{\lambda - t}$                                                                   |
| Beta                | $(0,1)$                                | $\alpha, \beta$     | $\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}$                                                                                                                  | $\frac{\alpha}{\alpha + \beta}$ | $\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$ | See mean formula for relative value of $\alpha$ and $\beta$. When values are large, pdf is concentrated around 0.5 (as opposed to extremeties).                                                                 |                                                                                                                                               |                                                                                              |                                                                                                 |
| Gamma               | $(0, \infty)$                          | $\alpha, \beta$     | $\frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}$                                                                                                        | $\frac{\alpha}{\beta}$          | $\frac{\alpha}{\beta^2}$                               | $\beta$ is known as the rate. The exponential and chi-squared distributions are special cases of the Gamma distribution. We can think of $\alpha$ as controlling the mean and $\beta$ controlling the variance. |                                                                                                                                               |                                                                                              | $\left(\frac{\beta}{\beta-t}\right)^\alpha$                                                     |
| Inverse-Gamma       | $(0, \infty)$                          | $\alpha, \beta$     | $\frac{\beta^\alpha}{\Gamma(\alpha)} x^{-\alpha-1} e^{-\beta/x}$                                                                                                       | $\frac{\beta}{\alpha-1}$        | $\frac{\beta^2}{(\alpha-1)^2(\alpha-2)}$               | $\beta$ is known as the scale (we use the other parameterization here)                                                                                                                                          |                                                                                                                                               |                                                                                              |                                                                                                 |
| Normal              | $(-\infty,\infty)$                     | $\mu, \sigma^2$     | $\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\|x-\mu\|^2 /\left(2 \sigma^2\right)}$                                                                                            | $\mu$                           | $\sigma^2$                                             |                                                                                                                                                                                                                 | $\hat{\mu} = \bar{X}$, $\hat{\sigma^2}=\frac{\sum_i\left(X_i-\mu\right)^2}{n}$, $\hat{\sigma^2}=\frac{\sum_i\left(X_i-\bar{X}\right)^2}{n-1}$ | $\mathcal{I}(\mu)=\frac{1}{\sigma^2}, \mathcal{I}\left(\sigma^2\right)=\frac{1}{2 \sigma^4}$ | $\exp \left(\mu t+\frac{\sigma^2 t^2}{2}\right)$                                                |
| Chi-Squared         | $(0,\infty)$                           | $n$                 | $\frac{1}{2^{n / 2} \Gamma(n / 2)} x^{n / 2-1} e^{-x / 2}$                                                                                                             | $n$                             | $2n$                                                   | The sum of $n$ squared standard Normals.                                                                                                                                                                        |                                                                                                                                               |                                                                                              | $(1-2 t)^{-n/2}$                                                                                |
| Multivariate-Normal | $\mathbb{R}^p$                         | $\pmb{\mu, \Sigma}$ | $(2 \pi)^{-p / 2} \operatorname{det}(\pmb{\Sigma})^{-1 / 2} \exp \left(-\frac{1}{2}(\mathbf{x}-\pmb{\mu})^{\mathrm{T}} \pmb{\Sigma}^{-1}(\mathbf{x}-\pmb{\mu})\right)$ | $\pmb{\mu}$                     | $\pmb{\Sigma}$                                         |                                                                                                                                                                                                                 |                                                                                                                                               |                                                                                              | $\exp\left(\pmb{\mu}^{\top}\mathbf{t} + \frac12 \mathbf{t}^{\top}\pmb{\Sigma}\mathbf{t}\right)$ |
| Dirichlet           | $x_i \in [0,1]$ and $\sum_i^K x_i = 1$ | $\pmb{\alpha}$      | $\frac{1}{\mathrm{~B}(\pmb{\alpha})} \prod_{i=1}^K x_i^{\alpha_i-1}$                                                                                                   |                                 |                                                        | $\mathbf{x}$ are the probabilities to draw from the $K$ classes.   Probabilities tend to be higher for higher $\alpha_i$. When the overall $\alpha$ value is higher, we tend to get more even probabilities.    |                                                                                                                                               |                                                                                              |                                                                                                 |
| Laplace             | $(-\infty,\infty)$                     | $b, \mu$            | $\frac{1}{2b}\exp\left(-\frac{\|x-\mu\|}{b}\right)$                                                                                                                    |                                 |                                                        | The double exponential - splice two exponentials around $\mu$.                                                                                                                                                  |                                                                                                                                               |                                                                                              |                                                                                                 |                                                                                                 |
| Dirac               |                                        |                     |                                                                                                                                                                        |                                 |                                                        |                                                                                                                                                                                                                 |                                                                                                                                               |                                                                                              |                                                                                                 |
| Pareto              | $(\beta,\infty)$                       | $\alpha, \beta$     | $\frac{\alpha\beta^\alpha}{x^{\alpha+1}}$                                                                                                                              |                                 |                                                        |                                                                                                                                                                                                                 |                                                                                                                                               |                                                                                              |                                                                                                 |

### Conjugate priors

| Name                  | Prior         | Likelihood  | Posterior     | Interpretation | Posterior Predictive | 
|-----------------------|---------------|-------------|---------------|----------------|----------------------|
| Beta-Binomial         | Beta          | Binomial    | Beta          |                |                      |
| Gamma-Poisson         | Gamma         | Poisson     | Gamma         |                |                      |
| Dirichlet-Categorical | Dirichlet     | Categorical | Dirichlet     |                |                      |
| Normal-Normal         | Normal        | Normal      | Normal        |                |                      |
| Inverse Gamma-Normal  | Inverse Gamma | Normal      | Inverse Gamma |                |                      |

### Uninformative priors
* One common uninformative prior is the Jeffrey's Prior, $p(\theta) \propto |I(\theta)|^{1/2}$
  * The key characteristic of this prior is that it is "invariant" under reparametrization. I found this statement confusing and [this video](https://www.youtube.com/watch?v=S42N_6pQ5TA) was very helpful for my understanding.
    * Note that this seems necessary for an uninformative prior, but I'm not sure about its sufficiency. On this topic, I like [Salegg Apfelton's response](https://stats.stackexchange.com/questions/7519/why-are-jeffreys-priors-considered-noninformative). $\mathcal{I}(\theta)$ is large when there are fewer values of $X$ that would be compatible with $\theta$. Therefore, it is "easier" to find evidence against these values, which implies lower posterior probabilities. Jeffrey's prior counteracts this effect by increasing the prior density for those values of $\theta$.


