# Classical-and-Modern-Machine-Learning
I am currently exploring a career pivot into Machine Learning roles. This repository will be used to help me consolidate my knowledge.

Broadly, this repository will include: 
* Notes on various topics
  * Classical ML/Statistics
    * Linear Algebra
    * Probability and Information Theory
    * Bayesian Statistics
    * Performance Metrics
    * Linear Regression & Regularization
    * Naive Bayes & Logistic Regression
    * GLMs
    * SVMs
    * Decision Trees
    * Ensemble Learning, Random Forests and Boosting
    * Dimensionality Reduction
    * K-means
    * Gaussian Mixtures and EM
    * Causal Inference
  * Modern ML (Note that the delineation is not super clear here and this separation is purely made for organizational purposes
    * Training
    * Practical Considerations
    * AI Safety
    * MLP
    * CNN Architectures
    * Autoencoders
    * GANs
    * Diffusion Models
    * Normalizing Flows
    * RNNs & LSTMs
    * Attention & Transformers
    * Modern LLMs
    * Reinforcement Learning
    * Applied AI
* Code implementation for various algorithms, which may either be implemented from scratch or copied. Any copied code will only be included after thorough understanding.
  * The first priority would be to get implementations that work.
  * If time permits, a stretch goal would be to refactor the code appropriately, e.g. [(John's repo)]([experiments/nlp_experiments/3.3%20Chat%20with%20Mixtral%20(fine-tuned).ipynb](https://github.com/johnma2006/candle/tree/main))
  * Other projects in my Github included projects from college where I implemented different ML models. These models should all be included in this repo. 
  * I want to emphasize that the goal of these implementations would be to ensure that I have a thorough understanding of topics, rather than being the most involved projects. Notebooks may be more sparse if I believe my understanding to be sufficient.
* Exercises for practice

I shall try to be diligent in citing my sources. Due to visa-related time constraints, I do apologize for any lapses in citation. At the current moment, I envision pulling most heavily from the following sources:
* [(Hands-On Machine Learning by Aurélien Géron)]([experiments/nlp_experiments/3.3%20Chat%20with%20Mixtral%20(fine-tuned).ipynb](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1098125975])
* [(Dive into Deep Learning by Zhang, Lipton, Li and Smola)]([http://d2l.ai])
* [(Deep Learning by Goodfellow, Bengio and Courville)]([https://www.deeplearningbook.org])
* [(Implementations of Various LLMs by Ma)]([https://github.com/johnma2006/candle])
* [(Distill)]([https://distill.pub])
* [(Lilian Weng's Blog)]([https://lilianweng.github.io])
* [(Christopher Olah's Blog)]([https://colah.github.io/about.html])
